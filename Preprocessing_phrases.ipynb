{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913a3b75-e48c-4e92-8891-210218571fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imutils\n",
    "import dlib \n",
    "import cv2 \n",
    "import imageio\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef8a557-9cfd-4e3c-af50-875c41577152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "\n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "    \tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fd95d-8433-444e-8e65-96c56e8f7045",
   "metadata": {},
   "source": [
    "rect_to_bb: Function to convert a bounding box predicted by dlib to (x, y, w, h) format as used in OpenCV\n",
    "\n",
    "rect: dlib rectangle objec\n",
    "\n",
    "shape_to_np: Function to convert facial landmark coordinates from dlib's shape predictor to NumPy array format\n",
    "shape: dlib shape object, representing facial landmarks# dtype: data type of the output NumPy array, default is 'int't\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81707fbb-b7ab-48e4-a7bd-baba639e843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_image(img, img_path, write_img_path, img_name):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('./MIRACL-VC1_all_in_one/shape_predictor_68_face_landmarks.dat')\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    if len(rects) > 1:\n",
    "    \tprint( \"ERROR: more than one face detected\")\n",
    "    \treturn\n",
    "    if len(rects) < 1:\n",
    "    \tprint( \"ERROR: no faces detected\")\n",
    "    \treturn\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        name, i, j = 'mouth', 48, 68\n",
    "        clone = gray.copy()\n",
    "\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))        \n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi = imutils.resize(roi, width = 250, inter=cv2.INTER_CUBIC)        \n",
    "        print('cropped/' + write_img_path)\n",
    "        imageio.imwrite('cropped/' + write_img_path, roi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d86dfa6-ee90-4d91-adb3-9fb611466697",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./MIRACL-VC1_all_in_one')\n",
    "predictor = dlib.shape_predictor('./MIRACL-VC1_all_in_one/shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8128ba38-df80-4d1c-8263-9d4d550d3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "people = ['F01','F02','F04','F05','F06','F07','F08','F09', 'F10','F11','M01','M02','M04','M07','M08']\n",
    "data_types = ['phrases']\n",
    "folder_enum = ['01','02','03','04','05','06','07','08','09','10']\n",
    "instances = ['01','02','03','04','05','06','07','08','09','10']\n",
    "\n",
    "words = ['Stop navigation', 'Excuse me', 'I am sorry', 'Thank you', 'Good bye', 'I love this game', 'Nice to meet you', 'You are welcome', 'How are you', 'Have a good time']          \n",
    "words_di = {i:words[i] for i in range(len(words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b82e28-ac42-4e06-977b-73d1d47067d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crearting new folder for cropped images\n",
    "#if not os.path.exists('cropped'):\n",
    "    #os.mkdir('cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb78fb5b-dd62-49c8-a3a8-364981ddc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def crop_one_person():      \n",
    "    \n",
    "#    people = ['F01','F02','F04','F05','F06','F07','F08','F09', 'F10','F11','M01','M02','M04','M07','M08']\n",
    "#    data_types = ['words']\n",
    "#   folder_enum = ['01','02']\n",
    "#    instances = ['01','02']\n",
    "\n",
    "    i = 1\n",
    "    for person_ID in people:\n",
    "        if not os.path.exists('cropped/' + person_ID ):\n",
    "            os.mkdir('cropped/' + person_ID + '/')\n",
    "\n",
    "        for data_type in data_types:\n",
    "            if not os.path.exists('cropped/' + person_ID + '/' + data_type):\n",
    "                os.mkdir('cropped/' + person_ID + '/' + data_type)\n",
    "\n",
    "            for phrase_ID in folder_enum:\n",
    "                if not os.path.exists('cropped/' + person_ID + '/' + data_type + '/' + phrase_ID):\n",
    "                    # F01/phrases/01\n",
    "                    os.mkdir('cropped/' + person_ID + '/' + data_type + '/' + phrase_ID)\n",
    "\n",
    "                for instance_ID in instances:\n",
    "                    # F01/phrases/01/01\n",
    "                    directory = './MIRACL-VC1_all_in_one/' + person_ID + '/' + data_type + '/' + phrase_ID + '/' + instance_ID + '/'\n",
    "                    dir_temp = person_ID + '/' + data_type + '/' + phrase_ID + '/' + instance_ID + '/'\n",
    "                    print(directory)\n",
    "                    filelist = os.listdir(directory)\n",
    "                    if not os.path.exists('cropped/' + person_ID + '/' + data_type + '/' + phrase_ID + '/' + instance_ID):\n",
    "                        os.mkdir('cropped/' + person_ID + '/' + data_type + '/' + phrase_ID + '/' + instance_ID)\n",
    "\n",
    "                        for img_name in filelist:\n",
    "                            if img_name.startswith('color'):\n",
    "                                image = imageio.imread(directory + '' + img_name)\n",
    "                                crop_and_save_image(image, directory + '' + img_name,\n",
    "                                                    dir_temp + '' + img_name, img_name)\n",
    "\n",
    "    print(f'Iteration : {i}')\n",
    "    i += 1\n",
    "  #  shutil.rmtree('cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbdd94f-9155-43d9-a8ec-6d48411d59ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MIRACL-VC1_all_in_one/F01/phrases/01/01/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satvi\\AppData\\Local\\Temp\\ipykernel_14836\\1483237364.py:35: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(directory + '' + img_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped/F01/phrases/01/01/color_001.jpg\n",
      "cropped/F01/phrases/01/01/color_002.jpg\n",
      "cropped/F01/phrases/01/01/color_003.jpg\n",
      "cropped/F01/phrases/01/01/color_004.jpg\n",
      "cropped/F01/phrases/01/01/color_005.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m      5\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mcrop_one_person\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m     times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (t2 \u001b[38;5;241m-\u001b[39m t1)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mcrop_one_person\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m img_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     35\u001b[0m                             image \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m img_name)\n\u001b[1;32m---> 36\u001b[0m                             \u001b[43mcrop_and_save_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mdir_temp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mcrop_and_save_image\u001b[1;34m(img, img_path, write_img_path, img_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop_and_save_image\u001b[39m(img, img_path, write_img_path, img_name):\n\u001b[0;32m      2\u001b[0m     detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m----> 3\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./MIRACL-VC1_all_in_one/shape_predictor_68_face_landmarks.dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# load the input image, resize it, and convert it to grayscale\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "os.mkdir('cropped')\n",
    "times = 0\n",
    "for _ in range(7):\n",
    "    t1 = time.time()\n",
    "    crop_one_person()\n",
    "    t2 = time.time()\n",
    "    times += (t2 - t1)\n",
    "\n",
    "print(\"Average time over 7 iterations : \", times/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f94e38-c553-4bb0-96a2-348fb9552c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac19c5-050e-49a5-b97a-dd7b027ba776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
